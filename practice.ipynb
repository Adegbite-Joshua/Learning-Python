{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j_a12Z6cOJXu"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "import json\n",
        "import base64\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import os\n",
        "\n",
        "# try:\n",
        "#   import torchinfo\n",
        "#   # import torchvision\n",
        "# except:\n",
        "#   !pip install torchinfo torchvision\n",
        "#   # import torchvision\n",
        "#   import torchinfo\n",
        "\n",
        "# from torchinfo import summary\n",
        "# from torchvision import datasets, transforms\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Glee31XDWstI"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RFlIk96ECZS"
      },
      "outputs": [],
      "source": [
        "# Downoad the datasets for training and testing from CIFAR10\n",
        "# train_datasets = datasets.CIFAR10(root=\"data\",\n",
        "#                                   train=True,\n",
        "#                                   transform=train_transform,\n",
        "#                                   download=True)\n",
        "# test_datasets = datasets.CIFAR10(root=\"data\",\n",
        "#                                   train=False,\n",
        "#                                   transform=test_transform,\n",
        "#                                  download=True)\n",
        "\n",
        "# class_names = train_datasets.classes\n",
        "\n",
        "# print(len(train_datasets), len(test_datasets))\n",
        "\n",
        "# # Define the number of samples you want to keep\n",
        "# subset_length = 20000  # For example, let's keep 10,000 samples\n",
        "\n",
        "# # Create subsets of the original datasets\n",
        "# train_datasets = Subset(train_datasets, range(subset_length))\n",
        "# test_datasets = Subset(test_datasets, range(subset_length))\n",
        "# print(len(train_datasets), len(test_datasets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy9eQBUyHfwF"
      },
      "outputs": [],
      "source": [
        "# # Create dataloader\n",
        "\n",
        "# BATCH_SIZE=256\n",
        "\n",
        "# train_dataloader = DataLoader(dataset=train_datasets,batch_size=BATCH_SIZE,shuffle=True,num_workers=os.cpu_count())\n",
        "# test_dataloader = DataLoader(dataset=test_datasets,batch_size=BATCH_SIZE,shuffle=False,num_workers=os.cpu_count())\n",
        "\n",
        "# print(\"lengths:\", len(train_dataloader),\" , \", len(test_dataloader))\n",
        "# print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwJNeLlMZnEM"
      },
      "outputs": [],
      "source": [
        "# # Create the classification model\n",
        "# class Model0(nn.Module):\n",
        "#   def __init__(self, input_shape, hidden_units, output_shape):\n",
        "#     super().__init__()\n",
        "#     self.conv_block_1 = nn.Sequential(\n",
        "#         nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.MaxPool2d(kernel_size=2, padding=1)\n",
        "#     )\n",
        "#     self.conv_block_2 = nn.Sequential(\n",
        "#         nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.MaxPool2d(kernel_size=2, padding=1)\n",
        "#     )\n",
        "#     self.conv_block_3 = nn.Sequential(\n",
        "#         nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.MaxPool2d(kernel_size=2, padding=1)\n",
        "#     )\n",
        "#     self.classifier = nn.Sequential(\n",
        "#         nn.Flatten(),\n",
        "#         nn.Linear(in_features=hidden_units*57*57,\n",
        "#                   out_features=output_shape)\n",
        "#     )\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = self.conv_block_1(x)\n",
        "#     x = self.conv_block_2(x)\n",
        "#     # print(x.shape)\n",
        "#     x = self.classifier(x)\n",
        "#     return x\n",
        "\n",
        "\n",
        "\n",
        "# class Model0(nn.Module):\n",
        "#   def __init__(self, input_shape, hidden_units, output_shape):\n",
        "#     super().__init__()\n",
        "#     self.conv_block_1 = nn.Sequential(\n",
        "#         nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(hidden_units),  # Batch Normalization\n",
        "#         nn.MaxPool2d(kernel_size=2, padding=1)\n",
        "#     )\n",
        "#     # Additional Convolutional Layers\n",
        "#     self.conv_block_2 = nn.Sequential(\n",
        "#         nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units*2, kernel_size=3, stride=1, padding=1),  # Increase in the number of filters\n",
        "#         nn.ReLU(),\n",
        "#         nn.Conv2d(in_channels=hidden_units*2, out_channels=hidden_units*2, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(hidden_units*2),  # Batch Normalization\n",
        "#         nn.MaxPool2d(kernel_size=2, padding=1)\n",
        "#     )\n",
        "#     # Another Convolutional Layer with Dropout\n",
        "#     self.conv_block_3 = nn.Sequential(\n",
        "#         nn.Conv2d(in_channels=hidden_units*2, out_channels=hidden_units*4, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.Conv2d(in_channels=hidden_units*4, out_channels=hidden_units*4, kernel_size=3, stride=1, padding=1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(hidden_units*4),  # Batch Normalization\n",
        "#         nn.MaxPool2d(kernel_size=2, padding=1)\n",
        "#     )\n",
        "#     self.classifier = nn.Sequential(\n",
        "#         nn.Flatten(),\n",
        "#         nn.Linear(in_features=hidden_units*4*29*29, out_features=hidden_units*4),  # Adjusted output shape based on the preceding layers\n",
        "#         nn.ReLU(),\n",
        "#         nn.Dropout(0.5),  # Dropout Regularization\n",
        "#         nn.Linear(in_features=hidden_units*4, out_features=output_shape)  # Output Layer\n",
        "#     )\n",
        "\n",
        "#   # Forward Pass\n",
        "#   def forward(self, x):\n",
        "#     x = self.conv_block_1(x)\n",
        "#     x = self.conv_block_2(x)\n",
        "#     x = self.conv_block_3(x)\n",
        "#     x = self.classifier(x)\n",
        "#     return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpjFcOgBM_eu"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model_0 = Model0(\n",
        "#     input_shape=3,\n",
        "#     hidden_units=10,\n",
        "#     output_shape=len(class_names)\n",
        "# ).to(device)\n",
        "# # model_0 = torch.load(\"/content/model_0_retrained2.pt\", map_location=device)\n",
        "# model_0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating model_1 using pretrained model**"
      ],
      "metadata": {
        "id": "gtbrQZi_AH6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the weights of the pretrained model\n",
        "weights = torchvision.models.EfficientNet_B1_Weights.DEFAULT # .DEFAULT = best available weights\n",
        "# model_1 = torchvision.models.efficientnet_b1(weights=weights).to(device)\n",
        "model_1 = torch.jit.load('pretrained_model2.pt', map_location=device)\n",
        "\n",
        "#auto_transforms = weights.transforms()\n",
        "auto_transforms = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    weights.transforms()\n",
        "])\n",
        "test_auto_transforms = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    #transforms.ToTensor(),\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "auto_transforms"
      ],
      "metadata": {
        "id": "gPR4KLsc_k4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downoad the datasets for training and testing from CIFAR10\n",
        "train_datasets = datasets.CIFAR10(root=\"pretrained_data\",\n",
        "                                  train=True,\n",
        "                                  transform=auto_transforms,\n",
        "                                  download=True)\n",
        "test_datasets = datasets.CIFAR10(root=\"pretrained_data\",\n",
        "                                  train=False,\n",
        "                                  transform=auto_transforms,\n",
        "                                  download=True)\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "\n",
        "print(len(train_datasets), len(test_datasets))\n",
        "\n",
        "# Define the number of samples you want to keep\n",
        "subset_length = 20000  # For example, let's keep 10,000 samples\n",
        "\n",
        "# Create subsets of the original datasets\n",
        "train_datasets = Subset(train_datasets, range(subset_length))\n",
        "# test_datasets = Subset(test_datasets, range(subset_length))\n",
        "print(len(train_datasets), len(test_datasets))"
      ],
      "metadata": {
        "id": "dg-HAsHkCIRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8nAZ6Eg_gj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWe_xfXAezUE"
      },
      "outputs": [],
      "source": [
        "summary(model_1, input_size=[256,3,224,224])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
        "for param in model_1.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "PufX5C2RBJKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Get the length of class_names (one output unit for each class)\n",
        "output_shape = len(class_names)\n",
        "\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "model_1.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=1280,\n",
        "                    out_features=output_shape, # same number of output units as our number of classes\n",
        "                    bias=True)).to(device)"
      ],
      "metadata": {
        "id": "-03lMDQMBcwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN1z5eNORepI"
      },
      "outputs": [],
      "source": [
        "# Create train and test step\n",
        "\n",
        "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_pred = model(X)\n",
        "\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    # print(f\"(y_pred_class == y): {(y_pred_class == y)}\")\n",
        "    # print(f\"(y_pred_class == y).sum(): {(y_pred_class == y).sum()}\")\n",
        "    # print(f\"(y_pred_class == y).sum().item(): {(y_pred_class == y).sum().item()}\")\n",
        "    # print(f\"(y_pred): {(y_pred)}\")\n",
        "    # print(f\"(y_pred_class == y).sum().item()/len(y_pred): {(y_pred_class == y).sum().item()/len(y_pred)}\")\n",
        "    train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  train_loss /= len(dataloader)\n",
        "  train_acc /= len(dataloader)\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "\n",
        "# Create train and test step\n",
        "\n",
        "def test_step(model, dataloader, loss_fn, device):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      y_pred = model(X)\n",
        "\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      # print(f\"(y_pred_class == y): {(y_pred_class == y)}\")\n",
        "      # print(f\"(y_pred_class == y).sum(): {(y_pred_class == y).sum()}\")\n",
        "      # print(f\"(y_pred_class == y).sum().item(): {(y_pred_class == y).sum().item()}\")\n",
        "      # print(f\"(y_pred): {(y_pred)}\")\n",
        "      # print(f\"(y_pred_class == y).sum().item()/len(y_pred): {(y_pred_class == y).sum().item()/len(y_pred)}\")\n",
        "      test_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  test_loss /= len(dataloader)\n",
        "  test_acc /= len(dataloader)\n",
        "\n",
        "  return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloader\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_datasets,batch_size=BATCH_SIZE,shuffle=True,num_workers=os.cpu_count())\n",
        "test_dataloader = DataLoader(dataset=test_datasets,batch_size=BATCH_SIZE,shuffle=False,num_workers=os.cpu_count())\n",
        "\n",
        "print(\"lengths:\", len(train_dataloader),\" , \", len(test_dataloader))\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "zQYj5em8pPyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGcPGlHVeXZM"
      },
      "outputs": [],
      "source": [
        "# Create train() function to train the model\n",
        "\n",
        "def train_model(model, train_dataloader, test_dataloader, loss_fn, optimizer, epochs, device):\n",
        "\n",
        "  results = {\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": [],\n",
        "\n",
        "  }\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_step(model=model, dataloader=train_dataloader,loss_fn=loss_fn, optimizer=optimizer, device=device)\n",
        "    test_loss, test_acc = test_step(model=model, dataloader=test_dataloader,loss_fn=loss_fn, device=device)\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "    print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0edMCMIaowRm"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(train_datasets[100][0].permute(1,2,0))\n",
        "plt.title(class_names[train_datasets[100][1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htnj5ypYluPB"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.01)\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "start_time = timer()\n",
        "EPOCHS = 20\n",
        "\n",
        "model_1_results = train_model(model=model_1, train_dataloader=train_dataloader, test_dataloader=test_dataloader, loss_fn=loss_fn, optimizer=optimizer, epochs=EPOCHS, device=device)\n",
        "\n",
        "end_time = timer()\n",
        "\n",
        "print(f\"Total training time: {end_time-start_time:.4f} seconds\")\n",
        "model_scripted = torch.jit.script(model_1) # Export to TorchScript\n",
        "model_scripted.save('pretrained_model2.pt') # Save"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "\n",
        "\n",
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_logit = model_1(X)\n",
        "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)\n",
        "\n",
        "\n",
        "# See if torchmetrics exists, if not, install it\n",
        "try:\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
        "except:\n",
        "    !pip install -q torchmetrics -U mlxtend # <- Note: If you're using Google Colab, this may require restarting the runtime\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "\n",
        "# Import mlxtend upgraded version\n",
        "import mlxtend\n",
        "print(mlxtend.__version__)\n",
        "assert int(mlxtend.__version__.split(\".\")[1]) >= 19 # should be version 0.19.0 or higher\n",
        "\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# 2. Setup confusion matrix instance and compare predictions to targets\n",
        "confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                         target=torch.tensor(test_datasets.targets))\n",
        "\n",
        "# 3. Plot the confusion matrix\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy\n",
        "    class_names=class_names, # turn the row and column labels into class names\n",
        "    figsize=(10, 7)\n",
        ");"
      ],
      "metadata": {
        "id": "x6KIacYlPOX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCbnQFc-muvB"
      },
      "outputs": [],
      "source": [
        "# Plot loss curves\n",
        "def plot_loss_curves(results):\n",
        "  loss = results['train_loss']\n",
        "  test_loss = results['test_loss']\n",
        "\n",
        "  accuracy = results['train_acc']\n",
        "  test_accuracy = results['test_acc']\n",
        "\n",
        "  epochs = range(len(results[\"train_loss\"]))\n",
        "\n",
        "  plt.figure(figsize=(15, 7))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs, loss, label=\"train_loss\")\n",
        "  plt.plot(epochs, test_loss, label=\"test_loss\")\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot the accuracy\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
        "  plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results"
      ],
      "metadata": {
        "id": "dTaRH8HUznOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(model_1_results)"
      ],
      "metadata": {
        "id": "85150ZCYzaxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "id": "FIfEsq0UTaAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on custom images\n",
        "\n",
        "def pred_and_plot_image(model,\n",
        "                        image_path,\n",
        "                        class_names,\n",
        "                        device=device):\n",
        "  transform = auto_transforms\n",
        "  target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
        "  target_image /= 255\n",
        "\n",
        "  if transform:\n",
        "    target_image = transform(target_image)\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    target_image = target_image.unsqueeze(0)\n",
        "    target_image_pred = model(target_image.to(device))\n",
        "\n",
        "  target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "  target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "  plt.imshow(target_image.squeeze().permute(1,2,0))\n",
        "  if class_names:\n",
        "    title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob {target_image_pred_probs.max().cpu():.3f}\"\n",
        "  else:\n",
        "    title = f\"Pred: {target_image_pred_label} | Prob {target_image_pred_probs.max().cpu():.3f}\"\n",
        "\n",
        "  plt.title(title)\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "Qxymdh5lzi5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_scripted = torch.jit.script(model_0) # Export to TorchScript\n",
        "# model_scripted.save('model_scriptednew.pt') # Save"
      ],
      "metadata": {
        "id": "9Ij4f5Ri74II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the custom image\n",
        "pred_and_plot_image(model=model_1,\n",
        "                    image_path=\"/content/download (13).jpeg\",\n",
        "                    class_names=class_names,\n",
        "                    device=device)"
      ],
      "metadata": {
        "id": "3CKJH1x22CCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V3dkfa4827yE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8HYB9nI+HFExdz1NbJ8So"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}